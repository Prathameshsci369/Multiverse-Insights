import logging
import sys
import gc
import json
import os
import psutil
from llama_cpp import Llama
from preprocessing_8k import MultiFormatProcessor

# --- Configuration for Final Analysis ---
MODEL_PATH = "/home/anand/Downloads/Qwen-Qwen2.5-7B-Instruct-Q4_K_M.gguf"
#MODEL_PATH = "/workspaces/Multiverse-Insights/Qwen2.5-VL-7B-Instruct-Q5_K_M.gguf"
INPUT_TOKENS = 29000
OUTPUT_TOKENS = 4096
N_GPU_LAYERS = 0

# --- Resource Management Configuration ---
CPU_CORES_FOR_MODEL = 10
RAM_LIMIT_GB = 6

# Token buffer to ensure we stay within the context window
TOKEN_BUFFER = 2000  # Buffer to account for prompt overhead
MAX_INPUT_TOKENS = INPUT_TOKENS - TOKEN_BUFFER - OUTPUT_TOKENS  # Leave room for output

# --- End Configuration ---

def _load_model(model_path, n_ctx, n_gpu_layers, n_threads):
    """Loads the GGUF model with balanced performance settings."""
    print("\n[Final Analysis] Loading final analysis model... This may take a moment.")
    try:
        llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            verbose=False,
            n_threads=n_threads,       # Use specified cores
            n_batch=1024,               # Good balance for throughput
            f16_kv=True,                # Use half-precision for KV cache
            use_mmap=True,              # Memory-map for fast loading
            use_mlock=True,             # Lock in RAM for speed
        )
        print(f"[Final Analysis] Model loaded successfully using {n_threads} cores.")
        return llm
    except Exception as e:
        print(f"[Final Analysis] Error: Failed to load model from {model_path}. Details: {e}")
        sys.exit(1)

def _load_json_data(filepath):
    """Loads and parses a JSON file generated by preprocessing_8k.py."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"[Final Analysis] Successfully loaded chunked data from {filepath}")
            return data
    except Exception as e:
        print(f"[Final Analysis] Error reading file {filepath}: {e}")
    return None

def _get_analysis_messages(json_data, chunk_ids=None, extract_key_points_only=False):
    """
    Creates the system and user prompts for the final analysis.
    
    Args:
        json_data: The JSON data loaded from the preprocessing output
        chunk_ids: Optional list of specific chunk IDs to analyze. If None, analyze all chunks.
        extract_key_points_only: If True, extract only key points without full structure
    """
    # Extract chunks from the JSON data
    if 'chunks' not in json_data:
        logging.error("Invalid JSON structure: 'chunks' key not found")
        return None
    
    chunks = json_data['chunks']
    
    # If specific chunk IDs are provided, filter the chunks
    if chunk_ids is not None:
        chunks = [chunk for chunk in chunks if chunk['chunk_id'] in chunk_ids]
        if not chunks:
            logging.error(f"No chunks found with IDs: {chunk_ids}")
            return None
    
    # Combine content from all chunks
    joined_text = "\n\n" + "="*60 + "\n\n".join([
        f"[Chunk {chunk['chunk_id']} - Sources: {', '.join(chunk.get('source_types', ['unknown']))}]\n{chunk['content']}"
        for chunk in chunks
    ])
    
    # Get metadata for context
    metadata = json_data.get('metadata', {})
    source_types = metadata.get('source_types', [])
    total_chunks = metadata.get('chunks_created', len(chunks))
    
    if extract_key_points_only:
        # Use a more detailed prompt for extracting key points
        system_prompt = (
            "You are a specialized text analyst focused on extracting the most important information "
            "from the provided text. Extract key points, anomalies, and topics with sufficient detail "
            "to provide context for a later synthesis. Be comprehensive but concise, focusing on the most "
            "significant information and relationships between concepts."
        )
        
        user_prompt = f"""Extract the most important information from the following text:

TEXT TO ANALYZE:
{joined_text}

Provide your analysis in this format:

KEY POINTS:
- [First key point with brief explanation]
- [Second key point with brief explanation]
- [Third key point with brief explanation]
- [Fourth key point with brief explanation]
- [Fifth key point with brief explanation]
- [Sixth key point with brief explanation]
- [Seventh key point with brief explanation]
- [Eighth key point with brief explanation]

ANOMALIES:
- [First anomaly or unusual pattern with explanation]
- [Second anomaly or unusual pattern with explanation]
- [Third anomaly or unusual pattern with explanation]
(Or write "None detected" if no anomalies are found)

TOPICS:
- [First topic with brief description]
- [Second topic with brief description]
- [Third topic with brief description]
- [Fourth topic with brief description]
- [Fifth topic with brief description]
- [Sixth topic with brief description]

ENTITIES:
- [First entity]
- [Second entity]
- [Third entity]
- [Fourth entity]
- [Fifth entity]

RELATIONSHIPS:
- [First relationship between entities or concepts]
- [Second relationship between entities or concepts]
- [Third relationship between entities or concepts]

STOP AFTER THIS LINE.
Print this exact token after the final line:A
"""
    else:
        # Use the full structured prompt for final analysis
        system_prompt = (
            "You are a specialized text analyst. You must follow the user's "
            "instructions precisely, providing a structured analysis of the provided data "
            f"(which consists of {len(chunks)} chunks from {', '.join(source_types)} sources) in the exact "
            "format they request. You must stop at the indicated line."
        )
        
        user_prompt = f"""Analyze the following text and provide a structured analysis:

TEXT TO ANALYZE:
{joined_text}

Provide your analysis in this exact format:

### Executive Summary:
- [First main point with detailed explanation]
- [Second main point with detailed explanation]
- [Third main point with detailed explanation]
- [Fourth main point with detailed explanation]
- [Fifth main point with detailed explanation]
- [Sixth main point with detailed explanation]

### Sentiment:
**Positive:** [number]% - [brief explanation with evidence]
**Negative:** [number]% - [brief explanation with evidence]
**Neutral:** [number]% - [brief explanation with evidence]

### Topics:
- [First topic]
- [Second topic]
- [Third topic]
- [Fourth topic]
- [Fifth topic]

### Entities:
- [First entity]
- [Second entity]
- [Third entity]
- [Fourth entity]
- [Fifth entity]

### Relationships:
- **Entity1 -> Entity2:** [Description of relationship]
- **Entity3 -> Entity4:** [Description of relationship]
- **Entity5 -> Entity6:** [Description of relationship]

### Anomalies:
- [First anomaly or unusual pattern]
- [Second anomaly or unusual pattern]
- [Third anomaly or unusual pattern]
(Or write "None detected" if no anomalies are found)

### Controversy Score:
[number between 0.0 and 1.0]/1.0 - [detailed explanation of controversy level]

STOP AFTER THIS LINE.
Print this exact token after the final line:A
"""
    
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
def _run_analysis(llm, messages):
    """Runs a single, streaming analysis and prints/returns the output."""
    print("\n[Final Analysis] Assistant: Generating report... \n", end="", flush=True)
    full_response = ""
    try:
        response_stream = llm.create_chat_completion(
            messages=messages,
            max_tokens=OUTPUT_TOKENS,
            stream=True,
            stop=["STOP AFTER THIS LINE."] 
        )
        
        for chunk in response_stream:
            delta = chunk["choices"][0]["delta"]
            if "content" in delta:
                token = delta["content"]
                print(token, end="")
                sys.stdout.flush()
                full_response += token
        
        print()
        return full_response.strip()
    except Exception as e:
        print(f"\n[Final Analysis] An error occurred during analysis: {e}")
        return None

def display_analysis_results(analysis_text, container=None):
    """
    Display the analysis results with proper markdown formatting.
    
    Args:
        analysis_text (str): The raw analysis text from the LLM
        container: Optional Streamlit container to display the results in
    """
    import streamlit as st
    
    # If no container is provided, use the main st interface
    if container is None:
        container = st
    
    # Simply display the analysis text with markdown formatting
    # This will preserve the formatting from the LLM output
    container.markdown(analysis_text)

def process_files_and_analyze(input_patterns, output_file="all_chunks_combined.json", 
                             chunk_ids=None, display_in_streamlit=False, container=None):
    """
    Process files using MultiFormatProcessor and then analyze the chunks.
    
    Args:
        input_patterns: List of file patterns to process
        output_file: Path to save the processed chunks
        chunk_ids: Optional list of specific chunk IDs to analyze
        display_in_streamlit: Boolean flag indicating if the results should be displayed in Streamlit
        container: Optional Streamlit container to display the results in
    """
    # --- Pre-flight RAM Check ---
    system_mem = psutil.virtual_memory()
    available_gb = system_mem.available / 1024**3
    if available_gb < RAM_LIMIT_GB + 1: # Check if at least limit + 1GB is free
        logging.warning(f"Low system memory detected: {available_gb:.2f}GB available. "
                        f"The process may become unstable or fail if it exceeds the {RAM_LIMIT_GB}GB limit.")
    
    # 1. Process files using MultiFormatProcessor
    print("\n[Final Analysis] Processing input files...")
    processor = MultiFormatProcessor(output_file=output_file, max_tokens=8000)
    result = processor.process_files(input_patterns)
    
    if not result:
        logging.error("Failed to process input files")
        return "ERROR: Failed to process input files."
    
    # 2. Load the processed chunks
    json_data = _load_json_data(output_file)
    if not json_data:
        return f"ERROR: [Final Analysis] Failed to load processed chunks from {output_file}."
    
    # 3. Create prompts for analysis
    messages = _get_analysis_messages(json_data, chunk_ids)
    if not messages:
        return "ERROR: Failed to create analysis messages."
    
    # 4. Load the model with specified resources
    llm = _load_model(MODEL_PATH, INPUT_TOKENS, N_GPU_LAYERS, CPU_CORES_FOR_MODEL)
    
    # 5. Run the analysis
    final_report = _run_analysis(llm, messages)
    
    # 6. Handle None response gracefully
    if final_report is None:
        logging.error("Final analysis returned None response")
        final_report = "ERROR: Final analysis failed to generate a response."
    
    logging.info(f"Generated analysis report with {len(final_report)} characters")
    
    # 7. Display results in Streamlit if requested
    if display_in_streamlit:
        logging.info("Displaying analysis results in Streamlit")
        try:
            display_analysis_results(final_report, container)
        except Exception as e:
            logging.error(f"Error displaying analysis results: {str(e)}")
            import traceback
            logging.error(f"Traceback: {traceback.format_exc()}")
    else:
        logging.info("Not displaying results in Streamlit (display_in_streamlit=False)")
    
    # 8. Release resources and exit
    print("\n[Final Analysis] Analysis complete. Releasing model resources.")
    del llm
    print("[Final Analysis] Resources released.")
    
    return final_report

# --- Main Public Function ---
def combined_analysis(data, is_direct=False, display_in_streamlit=False, container=None, extract_key_points_only=False):
    """
    Orchestrates the final analysis of processed chunks or direct text.
    
    Args:
        data: Either a path to a JSON file with processed chunks, or a dictionary with direct analysis data
        is_direct: Boolean flag indicating if this is a direct analysis
        display_in_streamlit: Boolean flag indicating if the results should be displayed in Streamlit
        container: Optional Streamlit container to display the results in
        extract_key_points_only: If True, extract only key points without full structure
    """
    # --- Pre-flight RAM Check ---
    system_mem = psutil.virtual_memory()
    available_gb = system_mem.available / 1024**3
    if available_gb < RAM_LIMIT_GB + 1: # Check if at least limit + 1GB is free
        logging.warning(f"Low system memory detected: {available_gb:.2f}GB available. "
                        f"The process may become unstable or fail if it exceeds the {RAM_LIMIT_GB}GB limit.")
    
    if is_direct:
        # Handle direct analysis (existing logic)
        logging.info("Performing direct analysis on full text.")
        json_data = data
        messages = _get_analysis_messages(json_data, extract_key_points_only=extract_key_points_only)
    else:
        # Handle processed chunks analysis
        if isinstance(data, str):
            # If data is a string, treat it as a file path
            json_data = _load_json_data(data)
            if not json_data:
                return f"ERROR: [Final Analysis] Failed to load processed chunks from {data}."
        else:
            # If data is already a dictionary, use it directly
            json_data = data
        
        # Create prompts for analysis
        messages = _get_analysis_messages(json_data, extract_key_points_only=extract_key_points_only)
        if not messages:
            return "ERROR: Failed to create analysis messages."
    
    # Load the model with specified resources
    llm = _load_model(MODEL_PATH, INPUT_TOKENS, N_GPU_LAYERS, CPU_CORES_FOR_MODEL)
    
    # Run the analysis
    final_report = _run_analysis(llm, messages)
    
    # Handle None response gracefully
    if final_report is None:
        logging.error("Final analysis returned None response")
        final_report = "ERROR: Final analysis failed to generate a response."
    
    logging.info(f"Generated analysis report with {len(final_report)} characters")
    
    # Display results in Streamlit if requested
    if display_in_streamlit:
        logging.info("Displaying analysis results in Streamlit")
        try:
            display_analysis_results(final_report, container)
        except Exception as e:
            logging.error(f"Error displaying analysis results: {str(e)}")
            import traceback
            logging.error(f"Traceback: {traceback.format_exc()}")
    else:
        logging.info("Not displaying results in Streamlit (display_in_streamlit=False)")
    
    # Release resources and exit
    print("\n[Final Analysis] Analysis complete. Releasing model resources.")
    del llm
    print("[Final Analysis] Resources released.")
    
    return final_report

# New function for extracting key points only
def extract_key_points(data, is_direct=False):
    """
    Extract only key points from the data without the full structured analysis.
    
    Args:
        data: Either a path to a JSON file with processed chunks, or a dictionary with direct analysis data
        is_direct: Boolean flag indicating if this is a direct analysis
    """
    return combined_analysis(data, is_direct=is_direct, extract_key_points_only=True)
